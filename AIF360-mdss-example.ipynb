{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Automatic stratification of Tabular data using MDSS**\n",
    "\n",
    "The goal of AutoStrat is to identify sub-populations who, as a group, have outcomes that significantly diverge from the overall population.\n",
    "\n",
    "There are $\\prod_{m=1}^{M}\\left(2^{|X_{m}|}-1\\right)$ unique subgroups from a dataset with $M$ features, with each feature having $|X_{m}|$ discretized values, where a subgroup is any $M$-dimension Cartesian set product, between subsets of feature-values from each feature --- excluding the empty set. MDSS mitigates this computational hurdle by approximately identifing the most statistically divergent subgroup in linear time (rather than exponential).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have a Census-Income dataset of ~16K records where we observe a given individual represented by their census data as earning more than $50k/yr or not. \n",
    "\n",
    "Since the outcome is binary, we can use a Bernoulli scoring function. Other scoring functions eg.`Poisson` may be appropriate for your dataset depending on the parametric assumptions of the outcome. In scenarios where we do not wish to make any parametric assumptions a scoring function like `BerkJones` may be more appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the MDSS module and Bernoulli modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from aif360.detectors.mdss_detector import bias_scan\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the csv file containing the data from Github. Note that the data containes categorical features and continous features have been bineed into 10 bins. When applying autostrat it is important that categorical features are not one-hot encoded as this may result in subsets that have individuals belonging to two different categories. The cardinality of each feature also influences run-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native_country</th>\n",
       "      <th>age_bin</th>\n",
       "      <th>education_num_bin</th>\n",
       "      <th>hours_per_week_bin</th>\n",
       "      <th>capital_gain_bin</th>\n",
       "      <th>capital_loss_bin</th>\n",
       "      <th>observed</th>\n",
       "      <th>expectation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>17-27</td>\n",
       "      <td>1-8</td>\n",
       "      <td>40-44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>37-47</td>\n",
       "      <td>9</td>\n",
       "      <td>45-99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Local-gov</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>28-36</td>\n",
       "      <td>12-16</td>\n",
       "      <td>40-44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.236226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>37-47</td>\n",
       "      <td>10-11</td>\n",
       "      <td>40-44</td>\n",
       "      <td>7298-7978</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.236226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>?</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>17-27</td>\n",
       "      <td>10-11</td>\n",
       "      <td>1-39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    workclass      education       marital_status          occupation  \\\n",
       "0     Private           11th        Never-married   Machine-op-inspct   \n",
       "1     Private        HS-grad   Married-civ-spouse     Farming-fishing   \n",
       "2   Local-gov     Assoc-acdm   Married-civ-spouse     Protective-serv   \n",
       "3     Private   Some-college   Married-civ-spouse   Machine-op-inspct   \n",
       "4           ?   Some-college        Never-married                   ?   \n",
       "\n",
       "  relationship    race      sex  native_country age_bin education_num_bin  \\\n",
       "0    Own-child   Black     Male   United-States   17-27               1-8   \n",
       "1      Husband   White     Male   United-States   37-47                 9   \n",
       "2      Husband   White     Male   United-States   28-36             12-16   \n",
       "3      Husband   Black     Male   United-States   37-47             10-11   \n",
       "4    Own-child   White   Female   United-States   17-27             10-11   \n",
       "\n",
       "  hours_per_week_bin capital_gain_bin capital_loss_bin  observed  expectation  \n",
       "0              40-44                0                0         0     0.236226  \n",
       "1              45-99                0                0         0     0.236226  \n",
       "2              40-44                0                0         1     0.236226  \n",
       "3              40-44        7298-7978                0         1     0.236226  \n",
       "4               1-39                0                0         0     0.236226  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = pd.read_csv('https://gist.githubusercontent.com/Viktour19/b690679802c431646d36f7e2dd117b9e/raw/d8f17bf25664bd2d9fa010750b9e451c4155dd61/adult_autostrat.csv')\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that ALL the records have the same expectation which corresponds to the mean income status of the entire population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDSS API and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scoring` parameter specifies the scoring function to be used in the scan. These can be one of `Bernoulli`, `Gaussian`, `Poisson` or `BerkJones`.\n",
    "\n",
    "The `overpredicted` parameter specifies whether we want to scan for a subgroup which the model favors or disfavors. If `True`, we scan for a subgroup whose predictions are fovourable in comparison with the actual outcomes. If `False`, the converse is true.\n",
    "\n",
    "The `penalty` co-efficient allows us to adjust the complexity of the highest scoring subset. It can be thought of a regularization constant.\n",
    "In each iteration, we optimize over subsets of all the attributes and randomly initialize the values of each attribute. `num_iters` specifies the number of iterations and thus random initializations\n",
    "\n",
    "We will scan for individuals with significantly lower and higher income level than the overall average of `0.24`. We'll start with a low penalty value and observe the score and complexity of the subset we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = dff['observed']\n",
    "data = dff.drop(['observed','expectation'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll scan for individuals with significantly higher income level than the overall average of `0.24`. We start with a low penalty value and observe the subset we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'marital_status': [' Married-AF-spouse', ' Married-civ-spouse'], 'education_num_bin': ['10-11', '12-16'], 'occupation': [' Adm-clerical', ' Armed-Forces', ' Craft-repair', ' Exec-managerial', ' Prof-specialty', ' Protective-serv', ' Sales', ' Tech-support', ' Transport-moving'], 'age_bin': ['28-36', '37-47', '48-90']}, 1339.0438)\n"
     ]
    }
   ],
   "source": [
    "privileged_subset = bias_scan(data=data, observations=observed, scoring='Bernoulli', overpredicted=False,penalty=2)\n",
    "print(privileged_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our detected privileged group has a size of 3401, we observe 0.6601 as the average probability of earning >50k, but our populatin mean is 0.2362\n",
      "\n",
      "This is a multiplicative increase in the odds by 6.279065608991142\n"
     ]
    }
   ],
   "source": [
    "dff = data.copy()\n",
    "dff['observed'] = observed\n",
    "dff['probabilities'] = observed.mean()\n",
    "\n",
    "to_choose = dff[privileged_subset[0].keys()].isin(privileged_subset[0]).all(axis=1)\n",
    "temp_df = dff.loc[to_choose]\n",
    "\n",
    "print(\"Our detected privileged group has a size of {}, we observe {} as the average probability of earning >50k, but our populatin mean is {}\"\\\n",
    ".format(len(temp_df), np.round(temp_df['observed'].mean(),4), np.round(temp_df['probabilities'].mean(),4)))\n",
    "\n",
    "group_obs = temp_df['observed'].mean()\n",
    "group_prob = temp_df['probabilities'].mean()\n",
    "\n",
    "odds_mul = (group_obs / (1 - group_obs) /(group_prob /(1 - group_prob)))\n",
    "print()\n",
    "print(\"This is a multiplicative increase in the odds by {}\"\\\n",
    ".format(odds_mul))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, We'll scan for individuals with significantly lower income level than the overall average of `0.24`. We start with a low penalty value and observe the subset we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'capital_gain_bin': ['0', '114-2354', '2407-3273', '3325-4416'], 'education': [' 10th', ' 11th', ' 12th', ' 1st-4th', ' 5th-6th', ' 7th-8th', ' 9th', ' Assoc-acdm', ' Assoc-voc', ' Bachelors', ' HS-grad', ' Preschool', ' Some-college'], 'relationship': [' Not-in-family', ' Other-relative', ' Own-child', ' Unmarried'], 'capital_loss_bin': ['0', '1672-1876']}, 1312.9051)\n"
     ]
    }
   ],
   "source": [
    "unprivileged_subset = bias_scan(data=data, observations=observed, scoring='Bernoulli', overpredicted=True,penalty=2)\n",
    "print(unprivileged_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our detected unprivileged group has a size of 8074, we observe 0.0307 as the average probability of earning >50k, but our populatin mean is 0.2362\n",
      "\n",
      "This is a multiplicative decrease in the odds by 9.760041246741118\n"
     ]
    }
   ],
   "source": [
    "to_choose = dff[unprivileged_subset[0].keys()].isin(unprivileged_subset[0]).all(axis=1)\n",
    "temp_df = dff.loc[to_choose]\n",
    "\n",
    "print(\"Our detected unprivileged group has a size of {}, we observe {} as the average probability of earning >50k, but our populatin mean is {}\"\\\n",
    ".format(len(temp_df), np.round(temp_df['observed'].mean(),4), np.round(temp_df['probabilities'].mean(),4)))\n",
    "\n",
    "group_obs = temp_df['observed'].mean()\n",
    "group_prob = temp_df['probabilities'].mean()\n",
    "\n",
    "odds_mul = (group_obs / (1 - group_obs) /(group_prob /(1 - group_prob)))\n",
    "print()\n",
    "print(\"This is a multiplicative decrease in the odds by {}\"\\\n",
    ".format(1/odds_mul))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias scan using Multi-Dimensional Subset Scan (MDSS)\n",
    "\n",
    "\"Identifying Significant Predictive Bias in Classifiers\" https://arxiv.org/abs/1611.08292\n",
    "\n",
    "The goal of bias scan is to identify a subgroup(s) that has significantly more predictive bias than would be expected from an unbiased classifier. There are $\\prod_{m=1}^{M}\\left(2^{|X_{m}|}-1\\right)$ unique subgroups from a dataset with $M$ features, with each feature having $|X_{m}|$ discretized values, where a subgroup is any $M$-dimension\n",
    "Cartesian set product, between subsets of feature-values from each feature --- excluding the empty set. Bias scan mitigates this computational hurdle by approximately identifing the most statistically biased subgroup in linear time (rather than exponential).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform data to numeric for the model\n",
    "y = dff['observed']\n",
    "features = dff.drop(['observed'], axis = 1)\n",
    "X = features.copy()\n",
    "\n",
    "for feature in X.columns:\n",
    "    X[feature] = X[feature].astype('category').cat.codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a simple classifier to predict the probability of the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Results:\n",
      "Precison:  0.7693075898801598\n",
      "Recall:  0.6008840353614144\n",
      "F1:  0.6747445255474452\n",
      "AUC-ROC:  0.7725771202138797\n"
     ]
    }
   ],
   "source": [
    "def report(actual, predicted):\n",
    "    print('Precison: ', precision_score(actual, predicted))\n",
    "    print('Recall: ', recall_score(actual, predicted))\n",
    "    print('F1: ', f1_score(actual, predicted))\n",
    "    print('AUC-ROC: ', roc_auc_score(actual, predicted))\n",
    "\n",
    "model = GBC()\n",
    "\n",
    "model.fit(X, y)\n",
    "preds = pd.Series(model.predict(X))\n",
    "\n",
    "print('Global Results:')\n",
    "report(y, preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias scan\n",
    "In bias scan, we are using the predictions from the model as the expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anomalous Subgroup:\n",
      " {'education_num_bin': ['1-8', '10-11', '9'], 'capital_gain_bin': ['0', '2407-3273', '3325-4416', '4508-5178', '5455-7262']}\n"
     ]
    }
   ],
   "source": [
    "subset, _ = bias_scan(data=features, observations=y, \n",
    "          expectations=preds, scoring='Bernoulli', \n",
    "          overpredicted=False, penalty=5)\n",
    "\n",
    "print()\n",
    "print('Anomalous Subgroup:\\n', subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subgroup Results:\n",
      "Precison:  0.6510851419031719\n",
      "Recall:  0.2498398462524023\n",
      "F1:  0.3611111111111111\n",
      "AUC-ROC:  0.6142479460020183\n"
     ]
    }
   ],
   "source": [
    "to_choose = data[subset.keys()].isin(subset).all(axis=1)\n",
    "_y = y.loc[to_choose].copy()\n",
    "_preds = preds.loc[to_choose].copy()\n",
    "\n",
    "print()\n",
    "print('Subgroup Results:')\n",
    "report(_y, _preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our detected subgroup has a size of 11353, we observe 0.13749669690830618 as the mean outcome, but our model predicts 0.05276138465603805\n",
      "This is a multiplicative decrease in the odds by 2.129100892041427\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Our detected subgroup has a size of {}, we observe {} as the mean outcome, but our model predicts {}\"\\\n",
    ".format(len(_y), _y.mean(), _preds.mean()))\n",
    "\n",
    "group_obs = _y.mean()\n",
    "group_prob = _preds.mean()\n",
    "\n",
    "odds_mul = ((group_prob /(1 - group_prob) / group_obs / (1 - group_obs)))\n",
    "print(\"This is a multiplicative decrease in the odds by {}\"\\\n",
    ".format(1/odds_mul))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a simpler subset, we can increase the penalty. Think of this penalty as a regularization parameter. The higher the value, the simpler our subset gets but the less extreme our bias is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anomalous Subgroup:\n",
      " {'education_num_bin': ['1-8', '10-11', '9']}\n"
     ]
    }
   ],
   "source": [
    "subset, _ = bias_scan(data=features, observations=y, \n",
    "          expectations=preds, scoring='Bernoulli', \n",
    "          overpredicted=False, penalty=20)\n",
    "\n",
    "print()\n",
    "print('Anomalous Subgroup:\\n', subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subgroup Results:\n",
      "Precison:  0.7430639324487334\n",
      "Recall:  0.34471180749860103\n",
      "F1:  0.4709480122324159\n",
      "AUC-ROC:  0.6616167689303029\n"
     ]
    }
   ],
   "source": [
    "to_choose = data[subset.keys()].isin(subset).all(axis=1)\n",
    "_y = y.loc[to_choose].copy()\n",
    "_preds = preds.loc[to_choose].copy()\n",
    "\n",
    "print()\n",
    "print('Subgroup Results:')\n",
    "report(_y, _preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our detected subgroup has a size of 11704, we observe 0.15268284347231717 as the mean outcome, but our model predicts 0.07083048530416952\n",
      "This is a multiplicative decrease in the odds by 1.6971138377552293\n"
     ]
    }
   ],
   "source": [
    "print(\"Our detected subgroup has a size of {}, we observe {} as the mean outcome, but our model predicts {}\"\\\n",
    ".format(len(_y), _y.mean(), _preds.mean()))\n",
    "\n",
    "group_obs = _y.mean()\n",
    "group_prob = _preds.mean()\n",
    "\n",
    "odds_mul = ((group_prob /(1 - group_prob) / group_obs / (1 - group_obs)))\n",
    "print(\"This is a multiplicative decrease in the odds by {}\"\\\n",
    ".format(1/odds_mul))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "This notebook demonstrates how to discover systematic deviations in data or models for a binary classification case. There are more examples for other use cases in demo_mdss_detectors.ipynb found in the [AIF360 repo](https://github.com/Trusted-AI/AIF360) examples folder. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
